10) Audio quality analysis (paper-ready dataset characterization)

“Explain how rms_db, peak_amp, and clip_ratio are computed (formulae, thresholds). Are signals assumed in [-1,1]?”

“Does the tool sample all files or subset? Does it preprocess before measuring? Clarify.”

“Where is the --audio-quality flag parsed and how does it select folders/splits?”

11) Determinism, performance, and scalability

“Which parts are deterministic vs non-deterministic (GPU ops, random seeds, batching order)? Is there a seed flag?”

“Give me throughput numbers available from logs (files/sec) and identify bottlenecks (I/O vs embedding vs whisper).”

“Is batch size used consistently across enroll and inference? Where does it matter and where is it ignored?”

12) Failure handling and edge cases (operational quality)

“List all error handling paths: missing audio, decode failures, missing labels, empty waveform after trim, etc. What gets skipped vs fatal?”

“Do we log skipped files with reasons to a structured output (CSV/JSON)? If not, recommend where to add it.”

13) “Paper extraction” prompts (turn repo into manuscript-ready content)

“Write a Methods section outline using ONLY what’s implemented in the repo: ASR frontend, SID frontend, scoring backend, z-norm, evaluation protocol.”

“Generate a Reproducibility checklist: exact commands to reproduce baseline and each major experiment (preprocess off vs on, z-norm off vs on).”

“Extract all default hyperparameters and put them in a single table: preprocessing params, enrollment params, scoring params, z-norm params.”

14) Quick sanity checks (to catch hidden mismatches)

“Verify enrollment and inference preprocessing match when using PKL metadata. Show the exact code path and confirm no overrides happen silently.”

“Confirm that ASR and SID do not accidentally apply different resampling/normalization when preprocessing is enabled.”

“Check whether any modules still load audio directly (bypassing audio_io). If yes, list them and exact lines.”